"""
Brain Module implementation for ChimeraForge.

This module provides LLM-powered reasoning capabilities using Google's Gemini API.
"""

import os
import time
from dataclasses import dataclass
from typing import Optional
import google.generativeai as genai
from backend.core.event_bus import Event, EventBus, EventType
from backend.core.module_registry import ModuleRegistry


@dataclass
class BrainResponse:
    """
    Response generated by the Brain module.
    
    Attributes:
        text: The main response text
        speak: Text to be spoken by the Mouth module (optional)
        open_url: URL to be opened by the Tentacle module (optional)
        reasoning: Internal reasoning process (optional)
    """
    text: str
    speak: Optional[str] = None
    open_url: Optional[str] = None
    reasoning: Optional[str] = None


class BrainModule:
    """
    Reasoning module that interprets events and generates intelligent responses using LLM.
    
    The Brain module listens for VISION_EVENT events and generates contextual responses
    that may include speech output or web actions.
    """
    
    def __init__(self, event_bus: EventBus, registry: ModuleRegistry, api_key: Optional[str] = None):
        """
        Initialize the Brain module.
        
        Args:
            event_bus: Event bus for subscribing to and publishing events
            registry: Module registry to check enabled state
            api_key: Google Gemini API key (defaults to GEMINI_API_KEY env var)
        """
        self.event_bus = event_bus
        self.registry = registry
        self.module_id = "brain"
        
        # Initialize Gemini client
        api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("Gemini API key not provided and GEMINI_API_KEY environment variable not set")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.5-flash')
        
        # Rate limiting: track last processing time
        self.last_process_time = 0
        self.min_interval = 10.0  # Minimum 10 seconds between responses (avoid 429 quota limits)
        
        # Subscribe to VISION_EVENT from event bus
        self.event_bus.subscribe(self.module_id, self.on_event)
    
    async def on_event(self, event: Event) -> None:
        """
        Handle incoming events from the event bus.
        
        Args:
            event: Event to process
        """
        # Only process VISION_EVENT and AUDIO_EVENT when module is enabled
        if not self.registry.is_enabled(self.module_id):
            return
        
        if event.type == EventType.VISION_EVENT or event.type == EventType.HEARING_EVENT:
            # Rate limiting: skip if too soon after last response
            current_time = time.time()
            if current_time - self.last_process_time < self.min_interval:
                print(f"Brain: Skipping event (rate limited, {current_time - self.last_process_time:.1f}s since last)")
                return
            
            # Wrap processing in try-catch for error handling
            try:
                print(f"Brain: Processing {event.type}...")
                response = await self.generate_response(event)
                await self._publish_action(response)
                self.last_process_time = current_time
                print(f"Brain: Response generated successfully")
            except Exception as e:
                # Log error with context
                print(f"Error in Brain module processing event {event.id}: {type(e).__name__}: {e}")
                # Publish error event for graceful degradation
                await self._publish_error_event(
                    error_type="processing",
                    message=f"Brain module failed to process event ({event.type}): {str(e)}",
                    details={
                        "event_id": event.id,
                        "error": str(e),
                        "error_type": type(e).__name__
                    },
                    recoverable=True
                )
    
    async def generate_response(self, event: Event) -> BrainResponse:
        """
        Generate a contextual response using LLM reasoning.
        
        Args:
            event: Event containing detection results or speech input
            
        Returns:
            BrainResponse with text, optional speak, and optional open_url
            
        Raises:
            Exception: If LLM API call fails after error handling
        """
        prompt = ""
        detected = False
        
        if event.type == EventType.VISION_EVENT:
            # Extract vision event details
            payload = event.payload
            detected = payload.get("detected", False)
            object_type = payload.get("object_type")
            confidence = payload.get("confidence", 0.0)
            
            # Create prompt for LLM
            if detected:
                prompt = f"You are a spooky Halloween AI creature that just detected a {object_type}. Generate a brief, creepy greeting (1-2 sentences max). Be eerie and Halloween-themed. Just respond with the text to speak."
            else:
                prompt = "You are a spooky Halloween AI creature. You're looking around but see nothing. Generate a brief, eerie observation about the emptiness (1-2 sentences max). Be creepy and atmospheric. Just respond with the text to speak."
                
        elif event.type == EventType.HEARING_EVENT:
            # Extract hearing event details
            payload = event.payload
            text = payload.get("text", "")
            
            # Create prompt for LLM
            prompt = f"You are a spooky Halloween AI creature. A human just said to you: '{text}'. Respond to them in character. Be eerie, mysterious, but conversational. Keep it brief (1-2 sentences). Just respond with the text to speak."
            detected = True # Treat hearing as a detection so we always speak back
        
        # Call Gemini API with error handling
        try:
            response = await self.model.generate_content_async(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.8,
                    max_output_tokens=500,
                )
            )
            
            # Parse response
            try:
                response_text = response.text
            except ValueError:
                # Handle cases where response.text fails (e.g. multi-part content)
                if hasattr(response, 'parts'):
                    response_text = "".join([part.text for part in response.parts])
                elif hasattr(response, 'candidates') and response.candidates:
                    parts = response.candidates[0].content.parts
                    response_text = "".join([part.text for part in parts])
                else:
                    response_text = ""
                    print(f"Brain: Could not extract text from response: {response}")
            
            if not response_text:
                print(f"Brain: Response text is empty. Candidates: {response.candidates}")
                try:
                    print(f"Brain: Prompt feedback: {response.prompt_feedback}")
                except:
                    pass

            print(f"Brain: Gemini raw response: {response_text[:200]}...")  # Log first 200 chars
            
            # Try to parse as JSON, fallback to plain text
            import json
            try:
                # Try to extract JSON if it's wrapped in markdown code blocks
                if "```json" in response_text:
                    # Extract JSON from markdown code block
                    json_start = response_text.find("```json") + 7
                    json_end = response_text.find("```", json_start)
                    response_text = response_text[json_start:json_end].strip()
                elif "```" in response_text:
                    # Extract from generic code block
                    json_start = response_text.find("```") + 3
                    json_end = response_text.find("```", json_start)
                    response_text = response_text[json_start:json_end].strip()
                
                response_data = json.loads(response_text)
                print(f"Brain: Successfully parsed JSON response")
                return BrainResponse(
                    text=response_data.get("text", response_text),
                    speak=response_data.get("speak"),
                    open_url=response_data.get("open_url"),
                    reasoning=response_data.get("reasoning")
                )
            except json.JSONDecodeError as e:
                # If not valid JSON, use the text as-is for speaking
                print(f"Brain: Response not JSON, using as plain text (will speak it)")
                # Use the raw text as both text and speak
                return BrainResponse(
                    text=response_text,
                    speak=response_text if detected else None
                )
        
        except Exception as e:
            # Log error with full context
            print(f"ERROR calling Gemini API: {type(e).__name__}: {e}")
            import traceback
            print(f"Traceback: {traceback.format_exc()}")
            
            # Publish error event
            await self._publish_error_event(
                error_type="external_service",
                message=f"Gemini API call failed: {str(e)}",
                details={
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "vision_event_id": event.id
                },
                recoverable=True
            )
            # Return fallback response for graceful degradation
            fallback_text = "The Brain is experiencing difficulties. The creature's thoughts are clouded..."
            return BrainResponse(
                text=fallback_text,
                speak=fallback_text if detected else None
            )
    
    async def _publish_action(self, response: BrainResponse) -> None:
        """
        Publish a SYSTEM_ACTION event to the event bus.
        
        Args:
            response: BrainResponse to publish
        """
        # Build payload with speak and open_url fields
        payload = {
            "text": response.text
        }
        
        if response.speak:
            payload["speak"] = response.speak
        
        if response.open_url:
            payload["open_url"] = response.open_url
        
        if response.reasoning:
            payload["reasoning"] = response.reasoning
        
        # Create and publish event
        event = Event.create(
            source_module=self.module_id,
            type=EventType.SYSTEM_ACTION,
            payload=payload
        )
        
        await self.event_bus.publish(event)
    
    async def _publish_error_event(self, error_type: str, message: str, 
                                   details: dict, recoverable: bool = True) -> None:
        """
        Publish an ERROR event to the event bus.
        
        Args:
            error_type: Type of error (e.g., "validation", "processing", "external_service")
            message: Human-readable error message
            details: Additional error details
            recoverable: Whether the module can recover from this error
        """
        # Create error event
        event = Event.create(
            source_module=self.module_id,
            type=EventType.ACTION_ERROR,
            payload={
                "error_type": error_type,
                "message": message,
                "details": details,
                "recoverable": recoverable
            }
        )
        
        # Publish error event
        await self.event_bus.publish(event)
